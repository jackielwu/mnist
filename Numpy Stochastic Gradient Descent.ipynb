{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "#from dnn_app_utils_v3 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "train_x_orig, train_y = loadlocal_mnist(\n",
    "        images_path='train-images.idx3-ubyte', \n",
    "        labels_path='train-labels.idx1-ubyte')\n",
    "\n",
    "test_x_orig, test_y = loadlocal_mnist(\n",
    "        images_path='t10k-images.idx3-ubyte',\n",
    "        labels_path='t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(y):\n",
    "    num_classes = 10\n",
    "    num_examples = len(y)\n",
    "    one_hot = np.zeros((num_classes, num_examples))\n",
    "    one_hot[y, np.arange(num_examples)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def read(x):\n",
    "    return int(np.where(x == 1)[0])\n",
    "\n",
    "\n",
    "def sigmoid(Z):    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "\n",
    "def cross_entropy(logits, labels):\n",
    "    y = labels.argmax(axis=1)\n",
    "    m = y.shape[0]\n",
    "    p = softmax(logits)\n",
    "    log_likehood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likehood) / m\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "def delta_cross_entropy(logits, labels):\n",
    "    y = labels.argmax(axis=1)\n",
    "    m = y.shape[0]\n",
    "    grad = softmax(logits)\n",
    "    grad[range(m),y] -= 1\n",
    "    grad = grad / m\n",
    "    return grad\n",
    "\n",
    "\n",
    "def initialize_adam(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "\n",
    "    return v, s\n",
    "    \n",
    "    \n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def update_parameters_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                           beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = beta1 * v['dW' + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]\n",
    "        v['db' + str(l+1)] = beta1 * v['db' + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\n",
    "        \n",
    "        v_corrected['dW' + str(l+1)] = v['dW' + str(l+1)] / (1-beta1**t)\n",
    "        v_corrected['db' + str(l+1)] = v['db' + str(l+1)] / (1-beta1**t)\n",
    "        \n",
    "        s['dW' + str(l+1)] = beta2 * s['dW' + str(l+1)] + (1-beta2) * grads['dW' + str(l+1)]**2\n",
    "        s['db' + str(l+1)] = beta2 * s['db' + str(l+1)] + (1-beta2) * grads['db' + str(l+1)]**2\n",
    "        \n",
    "        s_corrected['dW' + str(l+1)] = s['dW' + str(l+1)] / (1-beta2**t)\n",
    "        s_corrected['db' + str(l+1)] = s['db' + str(l+1)] / (1-beta2**t)\n",
    "        \n",
    "        parameters['W' + str(l+1)] = parameters['W'+str(l+1)] - learning_rate * v_corrected['dW' + str(l+1)] / (np.sqrt(s_corrected['dW' + str(l+1)]) + epsilon)\n",
    "        parameters['b' + str(l+1)] = parameters['b'+str(l+1)] - learning_rate * v_corrected['db' + str(l+1)] / (np.sqrt(s_corrected['db' + str(l+1)]) + epsilon)\n",
    "        \n",
    "    return parameters, v, s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data\n",
    "train_x_orig = np.reshape(train_x_orig, (train_x_orig.shape[0],28,28))\n",
    "test_x_orig = np.reshape(test_x_orig, (test_x_orig.shape[0],28,28))\n",
    "\n",
    "#labels\n",
    "train_y = convert(train_y)\n",
    "test_y = convert(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n",
      "Number of testing examples: 10000\n",
      "Each image is of size: (28, 28, 1)\n",
      "train_x_orig shape: (60000, 28, 28)\n",
      "train_y shape: (10, 60000)\n",
      "test_x_orig shape: (10000, 28, 28)\n",
      "test_y shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 1)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 3. It's a 3 picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "first_image = np.array(train_x_orig[index], dtype='float')\n",
    "plt.imshow(first_image)\n",
    "print (\"y = \" + str(read(train_y[:,index])) + \". It's a \" +str(read(train_y[:,index])) +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (784, 60000)\n",
      "test_x's shape: (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (np.dot(dZ,A_prev.T))/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "# forward prop\n",
    "# Layer: Z -> relu\n",
    "# Final layer: Z -> softmax\n",
    "\n",
    "# cost func: softmax + loss\n",
    "# back prop: linear back, relu back, softmax back?\n",
    "# grad descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8,\n",
    "          num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    v,s = initialize_adam(parameters)\n",
    "    t = 0\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1)\n",
    "        A2, cache2 = linear_forward(A1, W2, b2)\n",
    "        \n",
    "        # Compute cost\n",
    "        #cost = compute_cost(A2, Y)\n",
    "        cost = cross_entropy(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        #dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        dA2 = softmax(A2) - Y\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_backward(dA2,cache2)\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1)\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        t = t + 1\n",
    "        parameters, v, s = update_parameters_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.3065283395532954\n",
      "Cost after iteration 100: 0.0006834049637804026\n",
      "Cost after iteration 200: 0.0004473883891986185\n",
      "Cost after iteration 300: 0.0004131744703003198\n",
      "Cost after iteration 400: 0.0003852966602176997\n",
      "Cost after iteration 500: 0.00028442977559038597\n",
      "Cost after iteration 600: 0.00021571377461905198\n",
      "Cost after iteration 700: 0.0001681239334860547\n",
      "Cost after iteration 800: 0.00013682622233763224\n",
      "Cost after iteration 900: 0.00010913811192090318\n",
      "Cost after iteration 1000: 8.801680024135021e-05\n",
      "Cost after iteration 1100: 7.280340706153426e-05\n",
      "Cost after iteration 1200: 6.04237764568936e-05\n",
      "Cost after iteration 1300: 5.113919971186531e-05\n",
      "Cost after iteration 1400: 4.408622716936703e-05\n",
      "Cost after iteration 1500: 3.790025847288088e-05\n",
      "Cost after iteration 1600: 3.300144563568278e-05\n",
      "Cost after iteration 1700: 2.887650171214305e-05\n",
      "Cost after iteration 1800: 2.5593652646503874e-05\n",
      "Cost after iteration 1900: 2.2944210351659582e-05\n",
      "Cost after iteration 2000: 2.065630093655478e-05\n",
      "Cost after iteration 2100: 1.8566174412745617e-05\n",
      "Cost after iteration 2200: 1.693333247071474e-05\n",
      "Cost after iteration 2300: 1.535251868476963e-05\n",
      "Cost after iteration 2400: 1.3979971675971944e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = model(train_x, train_y, layers_dims = (784, 500, 10), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters)\n",
    "    p = np.zeros((1,m))\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1)\n",
    "    A2, cache2 = linear_forward(A1, W2, b2)\n",
    "    test_loss = cross_entropy(A2, y)\n",
    "    pred = np.max(softmax(A2),axis=0,keepdims=1) == softmax(A2)\n",
    "    correct = np.sum(pred == y)\n",
    "    print(\"test loss = %f, test accuracy = %f\" %(test_loss, 100. * correct/m/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss = 0.000012, test accuracy = 100.000000\n",
      "test loss = 0.000147, test accuracy = 99.536000\n"
     ]
    }
   ],
   "source": [
    "predict(train_x, train_y, parameters)\n",
    "predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
